# EX.2024SO

## EX.2024SO.1

### Enunciado EX.2024SO.1

Dada una colección de variables aleatorias $X_1, X_2, X_3, X_4, X_5$, la densidad de probabilidad conjunta $p(X_1, X_2, X_3, X_4, X_5)$ es equivalente a $p(X_5, X_4, X_3, X_2, X_1)$:

a) En ningún caso.
b) Siempre.
c) Solo bajo condiciones específicas de dependencia.

### Solución EX.2024SO.1

b)

## EX.2024SO.2

### Enunciado EX.2024SO.2

Al realizar una transformación de una variable aleatoria $x$ a una nueva variable $y$ mediante $y = f(x)$, la distribución resultante $p(y)$:

a) Se determina en función del valor absoluto del determinante de la matriz Jacobiana de la transformación inversa $f^{-1}$.
b) Es inversamente proporcional a la matriz Jacobiana de la función directa $f$.
c) Se considera una práctica desaconsejada debido a la posible aparición de patologías en la nueva densidad.

### Solución EX.2024SO.2

a)

## EX.2024SO.3

### Enunciado EX.2024SO.3

La densidad conjunta de un grupo de variables aleatorias intercambiables (*exchangeable*) puede expresarse como el producto de distribuciones independientes e idénticamente distribuidas (i.i.d.):

a) Siempre que se satisfagan las condiciones del Teorema de de Finetti.
b) Única y exclusivamente si se cumplen las premisas de de Finetti.
c) Como una propiedad universal de cualquier conjunto de variables.

### Solución EX.2024SO.3

a)

## EX.2024SO.4

### Enunciado EX.2024SO.4

Dentro de un modelo normal jerárquico, la distribución posterior de las medias de cada grupo (condicionada a los hiperparámetros) se define como:

a) Una distribución Beta cuyos parámetros $\alpha$ y $\beta$ dependen del promedio ponderado por precisiones de la media grupal y global.
b) Una normal centrada en el promedio de la media del grupo y el hiperparámetro de la media global, utilizando las precisiones como factores de ponderación.
c) Una normal centrada en el promedio de la media del grupo y el hiperparámetro de la media global, utilizando las varianzas como factores de ponderación.

### Solución EX.2024SO.4

b)

## EX.2024SO.5

### Enunciado EX.2024SO.5

El valor esperado del logaritmo de la densidad predictiva para una observación futura ($\text{elpd}$) se define formalmente como:

a) $\int \log(p_{post}(\tilde y)) \cdot f(\tilde y) \, d\tilde y$
b) $\int \log(p_{post}(\tilde y \mid y)) \cdot f(\tilde y) \, d\tilde y$
c) $\int \log(p_{post}(\tilde y, \theta)) \cdot f(\tilde y) \, d\tilde y$

### Solución EX.2024SO.5

b)

## EX.2024SO.6

### Enunciado EX.2024SO.6

Cuando no se dispone de soluciones analíticas para la posterior de los parámetros $\theta$, la exactitud predictiva de un modelo puede estimarse mediante:

a) La aproximación:
$$\sum_{i=1}^{n} \log \left( \frac{1}{S} \sum_{s=1}^{S} p(y_i \mid \theta^s) \right)$$
basada en un conjunto de $S$ muestras obtenidas de la distribución posterior.
b) La sustitución de la distribución posterior por el producto de la verosimilitud y la marginal posterior.
c) Es un valor que no admite estimación ni cálculo aproximado en tales circunstancias.

### Solución EX.2024SO.6

a)

## EX.2024SO.7

### Enunciado EX.2024SO.7

La validación cruzada (*cross-validation*) es una técnica para evaluar la precisión predictiva que:

a) Es incompatible con los modelos jerárquicos bayesianos.
b) Intenta mitigar el sesgo derivado de evaluar el modelo con los mismos datos utilizados para su ajuste.
c) Se basa principalmente en la estimación del número efectivo de parámetros del modelo.

### Solución EX.2024SO.7

b)

## EX.2024SO.8

### Enunciado EX.2024SO.8

El Criterio de Información de la Desviación (DIC) se define técnicamente como:

a) -2 veces el logaritmo de la densidad predictiva evaluada en la media posterior, con un ajuste por sobreajuste cuya estimación puede ser negativa.
b) -2 veces el logaritmo de la densidad predictiva evaluada en la media posterior, con un ajuste por sobreajuste que no depende de los datos.
c) -2 veces el logaritmo de la densidad predictiva evaluada en la mediana posterior, con un ajuste por sobreajuste que puede ser negativo.

### Solución EX.2024SO.8

a)

## EX.2024SO.9

### Enunciado EX.2024SO.9

Para implementar un muestreador de Gibbs con el fin de generar muestras de la distribución posterior, es requisito indispensable:

a) Conocer las verosimilitudes marginales del modelo.
b) Que las cadenas de Markov resultantes presenten una correlación muy elevada.
c) Disponer de las distribuciones posteriores condicionales para cada parámetro (o bloque) dados los demás parámetros y los datos.

### Solución EX.2024SO.9

c)

## EX.2024SO.10

### Enunciado EX.2024SO.10

La justificación de que el algoritmo de Metropolis converge a la distribución posterior objetivo al alcanzar el estado estacionario se basa en:

a) Probar que las probabilidades de transición entre dos valores de los parámetros son simétricas independientemente de los datos.
b) Confirmar visualmente que diferentes cadenas de Markov convergen hacia el mismo punto.
c) Demostrar que la ratio de probabilidades posteriores entre dos estados sucesivos es independiente de la distribución de propuesta utilizada.

### Solución EX.2024SO.10

c)
