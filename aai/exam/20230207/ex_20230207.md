<!--
SPDX-FileCopyrightText: 2026 Colaboradores de apuntes_muicd_uned

SPDX-License-Identifier: CC-BY-4.0
-->

# AAI.EX.20230207

Ejercicios elaborados con fines educativos, inspirados en los contenidos evaluados en el exámen del 07/02/2023 (convocatoria Feb-2023) de Aprendizaje Automático 1 del MUICD de la UNED.

Este documento no es una copia ni una transcripción del examen oficial, sino una redacción propia de ejercicios conceptualmente equivalentes.

## Test

Pregunta correcta: +0.5 puntos  
Pregunta incorrecta: -0.1 puntos  

### AAI.EX.20230207.T.1

#### Enunciado AAI.EX.20230207.T.1

\
La técnica conocida como LLE (Local Linear Embedding) se caracteriza por ser:

a) Un método lineal de reducción de dimensionalidad basado en proyecciones globales.  
b) Un método lineal que preserva relaciones lineales globales entre instancias.  
c) Un método no lineal de reducción de dimensionalidad que preserva relaciones locales y funciona especialmente bien con datos en variedades curvas como el “rollo suizo”.  
d) Un método no lineal basado en proyecciones ortogonales.

#### Solución AAI.EX.20230207.T.1

\
Respuesta correcta: **c)**

Justificación: LLE es una técnica **no lineal** que preserva las relaciones locales entre vecinos cercanos, siendo muy adecuada para datos que viven en variedades no lineales.

---

### AAI.EX.20230207.T.2

#### Enunciado AAI.EX.20230207.T.2

\
¿Con qué objetivo se utiliza el conjunto de prueba (test) en un flujo estándar de aprendizaje automático?

a) Para estimar el error de generalización antes del despliegue en producción.  
b) Para elegir modelos y ajustar hiperparámetros.  
c) Para ambas cosas.  
d) Para ninguno de los fines anteriores.

#### Solución AAI.EX.20230207.T.2

\
Respuesta correcta: **a)**

Justificación: el conjunto de test se reserva para **evaluar el rendimiento final** del modelo; no debe usarse para selección ni ajuste, que se hacen con el conjunto de validación.

---

### AAI.EX.20230207.T.3

#### Enunciado AAI.EX.20230207.T.3

\
¿Por qué es importante escalar las variables de entrada al usar Máquinas de Vectores de Soporte (SVM)?

a) Para reducir el sobreajuste.  
b) Porque SVM es invariante a la escala.  
c) Para reducir el subajuste.  
d) Para evitar que las variables con valores pequeños queden infravaloradas frente a otras de mayor escala.

#### Solución AAI.EX.20230207.T.3

\
Respuesta correcta: **d)**

Justificación: SVM depende de distancias y productos escalares; si no se escalan los datos, las características con valores grandes dominan el modelo.

---

### AAI.EX.20230207.T.4

#### Enunciado AAI.EX.20230207.T.4

\
En un árbol de decisión, ¿cómo suele compararse la impureza de Gini de un nodo hijo con la de su nodo padre?

a) Normalmente es mayor.  
b) Normalmente es menor.  
c) Siempre es menor.  
d) No existe relación entre ambas.

#### Solución AAI.EX.20230207.T.4

\
Respuesta correcta: **b)**

Justificación: los criterios de división buscan **reducir la impureza**, por lo que, en general, los nodos hijos son más puros que su padre (aunque no siempre estrictamente).

---

### AAI.EX.20230207.T.5

#### Enunciado AAI.EX.20230207.T.5

\
¿Cuáles de las siguientes son razones habituales para aplicar reducción de dimensionalidad?

a) Acelerar y mejorar algoritmos posteriores.  
b) Aumentar el coste computacional.  
c) Facilitar la interpretación de los datos transformados.  
d) a) y c) son correctas.

#### Solución AAI.EX.20230207.T.5

\
Respuesta correcta: **d)**

Justificación: reducir dimensionalidad puede **mejorar eficiencia**, reducir ruido y facilitar la **visualización e interpretación**.

---

### AAI.EX.20230207.T.6

#### Enunciado AAI.EX.20230207.T.6

\
¿Qué algoritmos de clustering se basan en la detección de regiones de alta densidad?

a) BIRCH y K-means.  
b) Mean-Shift y DBSCAN.  
c) Mean-Shift y K-means.  
d) DBSCAN y BIRCH.

#### Solución AAI.EX.20230207.T.6

\
Respuesta correcta: **b)**

Justificación: **DBSCAN** y **Mean-Shift** identifican clusters como regiones densas separadas por zonas de baja densidad.

---

### AAI.EX.20230207.T.7

#### Enunciado AAI.EX.20230207.T.7

\
En un MLP con 10 entradas, una capa oculta de 50 neuronas y una capa de salida de 3 neuronas, ¿qué dimensiones tiene la matriz de salida $Y$?

a) $3 \times 50$  
b) $M \times 3$, siendo $M$ el tamaño del lote  
c) $3 \times M$, siendo $M$ el tamaño del lote  
d) $3 \times 10$

#### Solución AAI.EX.20230207.T.7

\
Respuesta correcta: **b)**

Justificación: cada fila de $Y$ corresponde a una instancia del lote y cada columna a una neurona de salida, por lo que $Y \in \mathbb{R}^{M \times 3}$.

---

### AAI.EX.20230207.T.8

#### Enunciado AAI.EX.20230207.T.8

\
En la clase `KernelDensity` del paquete `sklearn.neighbors`, ¿cuál es el kernel que se utiliza por defecto?

a) `tophat`  
b) `epanechnikov`  
c) `exponential`  
d) `gaussian`

#### Solución AAI.EX.20230207.T.8

\
Respuesta correcta: **d)**

Justificación: el kernel **gaussiano** es la opción por defecto en `KernelDensity`.

---

### AAI.EX.20230207.T.9

#### Enunciado AAI.EX.20230207.T.9

\
¿Bajar siempre en la dirección de mayor pendiente para llegar al fondo de un valle es una analogía clásica de qué algoritmo de optimización?

a) Descenso del gradiente.  
b) Algoritmo voraz.  
c) Optimización aleatoria.  
d) Algoritmo genético.

#### Solución AAI.EX.20230207.T.9

\
Respuesta correcta: **a)**

Justificación: el descenso del gradiente avanza iterativamente en la dirección de máxima pendiente descendente para minimizar una función.

---

### AAI.EX.20230207.T.10

#### Enunciado AAI.EX.20230207.T.10

\
En un problema de clasificación multiclase con pocos datos, atributos independientes y distribuciones normales, ¿qué algoritmo es más adecuado si se prioriza una inferencia muy rápida?

a) SVM.  
b) Árboles de decisión.  
c) Naive Bayes.  
d) Redes neuronales.

#### Solución AAI.EX.20230207.T.10

\
Respuesta correcta: **c)**

Justificación: **Naive Bayes** tiene inferencia extremadamente rápida y funciona bien bajo hipótesis de independencia y normalidad de las características.

**Paso B: estimación del precio semanal por producto (con modelo del temario)**:

Dado que la política de la empresa fija un único precio semanal por tipo de cítrico, el objetivo del Paso B es aprender una función:

$ \hat{p} = f(\text{semana}, \text{categoría}) $

donde la categoría usada será la predicha en el Paso A, $\hat{c}$.

**Modelo elegido (recomendado): Árbol de decisión para regresión (DT-regresión)**:

**Variables**:

- Entradas ($X$):
  - semana del año $s$ (numérica)
  - categoría $c$ (codificada, por ejemplo, one-hot)
- Salida ($y$):
  - precio/kg

**Justificación**:

- Es eficiente con grandes volúmenes de datos (entrenamiento y predicción rápidos).
- No requiere escalado.
- Captura patrones no lineales y “efectos por tramos” (por ejemplo, cambios bruscos de precio entre semanas).
- En este problema puede aproximar muy bien el comportamiento real, porque el precio depende solo de $(c,s)$.

**Diseño del proyecto**:

1) Construcción del dataset (histórico)
   - Para cada registro histórico: $(s, c) \rightarrow precio/kg$.
   - Aunque haya muchos registros por productor, todos comparten el mismo precio “oficial” por semana y categoría; el árbol aprenderá esa relación.

2) Preprocesamiento
   - Codificar la categoría (one-hot).
   - No es necesario escalado.
   - (Opcional) tratar la semana como variable cíclica si se desea (no imprescindible en árboles).

3) Validación
   - Separación por años si existen varios cursos (validación temporal).
   - Métricas: MAE y/o RMSE.

4) Entrenamiento
   - Ajustar un árbol de decisión de regresión.
   - Controlar sobreajuste con hiperparámetros:
     - profundidad máxima
     - mínimo de muestras por hoja

5) Predicción en el año actual
   - Para cada registro:
     1. Obtener categoría predicha $\hat{c}$ (Paso A).
     2. Calcular precio:
        $ \text{precio}_{\text{pred}} = f(s, \hat{c}) $

6) Salida “oficial” por semana y producto
   - Si se requiere un único precio por $(s, c)$, se puede:
     - predecir con una entrada representativa de cada par $(s,c)$, o
     - agregar predicciones de los registros de ese par (media/mediana).

**Alternativa del temario (si se quiere un enfoque por similitud): kNN-regresión**:

- Entradas: semana + categoría codificada.
- Idea: el precio se estima como promedio de los $k$ vecinos más cercanos.
- Requiere escalado si se mezclan variables con distinta escala.
- Puede ser más lento en predicción con grandes volúmenes que un árbol, por lo que DT-regresión suele ser preferible aquí.

**¿Es necesario inferir primero la categoría?**:

Sí, porque el precio depende explícitamente de la categoría:

- Sin categoría no se sabe qué función aplicar para esa semana.
- Clasificar primero permite además detectar incertidumbre: si el Paso A tiene baja confianza, se puede marcar el caso o usar una política conservadora.
