<!--
SPDX-FileCopyrightText: 2026 Colaboradores de apuntes_muicd_uned

SPDX-License-Identifier: CC-BY-4.0
-->

# AAII.EX.2023SO

Ejercicios elaborados con fines educativos, inspirados en los contenidos evaluados en el exámen de la sesión ordinaria de la convocatoria de septiembre 2023 de Aprendizaje Automático II del MUICD de la UNED.

Este documento no es una copia ni una transcripción del examen oficial, sino una redacción propia de ejercicios conceptualmente equivalentes.

Asignatura: Aprendizaje Automático II  
Duración máxima: 60 minutos  
Material permitido: todo tipo de material de consulta.

## AAII.EX.2023SO.1

### Enunciado AAII.EX.2023SO.1

El uso de bagging mejora la calidad de las predicciones principalmente porque…

- A. disminuye tanto la varianza como el sesgo.
- B. reduce la varianza manteniendo el sesgo aproximadamente constante.
- C. incrementa la varianza mientras reduce el sesgo.
- D. reduce la varianza a costa de aumentar el sesgo.

### Solución AAII.EX.2023SO.1

## AAII.EX.2023SO.2

### Enunciado AAII.EX.2023SO.2

En un bosque aleatorio, la relevancia de cada variable suele evaluarse…

- A. promediando su efecto sobre la precisión en todos los árboles.
- B. analizando la variación del error OOB asociada a cada variable.
- C. midiendo su impacto directo en la función de pérdida.
- D. observando los criterios de partición donde no fue utilizada.

### Solución AAII.EX.2023SO.2

## AAII.EX.2023SO.3

### Enunciado AAII.EX.2023SO.3

En un proceso iterativo de clasificación, el peso asignado a una muestra en cada ronda…

- A. disminuye cuando la muestra se clasifica incorrectamente.
- B. coincide con la magnitud absoluta de su valor objetivo.
- C. refleja la importancia de clasificarla correctamente en esa iteración.
- D. aumenta si la muestra ha sido mal clasificada.

### Solución AAII.EX.2023SO.3

## AAII.EX.2023SO.4

### Enunciado AAII.EX.2023SO.4

En gradient boosting, suponiendo $\gamma > 0$, la hipótesis de aprendizaje débil establece que el error de cada modelo base…

- A. no supera $2 - \gamma$.
- B. es mayor que $0.5 + \gamma$.
- C. es inferior a $0.5 - \gamma$.
- D. coincide exactamente con $0.5$.

### Solución AAII.EX.2023SO.4

## AAII.EX.2023SO.5

### Enunciado AAII.EX.2023SO.5

En clasificación, AdaBoost combina modelos débiles…

- A. promediando directamente sus predicciones.
- B. mediante votación simple.
- C. aplicando un discriminante lineal.
- D. utilizando una votación ponderada.

### Solución AAII.EX.2023SO.5

## AAII.EX.2023SO.6

### Enunciado AAII.EX.2023SO.6

Según la clasificación propuesta por Mohandes y colaboradores, la estrategia de combinación de clasificadores más habitual y efectiva se sitúa…

- A. en el nivel de sensores.
- B. en el nivel de variables o características.
- C. en el nivel de decisión.
- D. en el nivel de eficiencia computacional.

### Solución AAII.EX.2023SO.6

## AAII.EX.2023SO.7

### Enunciado AAII.EX.2023SO.7

El algoritmo K-medias puede ampliarse…

- A. reemplazando la métrica euclídea de los prototipos por otras más robustas frente a valores atípicos.
- B. modificando únicamente la actualización del codificador con métricas robustas.
- C. redefiniendo los prototipos del grupo con medidas menos sensibles a outliers.

### Solución AAII.EX.2023SO.7

## AAII.EX.2023SO.8

### Enunciado AAII.EX.2023SO.8

El índice de Dunn se define como…

- A. la relación entre dispersión entre grupos y dispersión interna normalizada.
- B. el cociente entre la mínima distancia intergrupo y la máxima distancia intragrupo.
- C. el logaritmo de la relación entre dispersión entre grupos e intragrupos.

### Solución AAII.EX.2023SO.8

## AAII.EX.2023SO.9

### Enunciado AAII.EX.2023SO.9

En un dendrograma de clustering jerárquico aglomerativo, la altura de cada fusión…

- A. refleja la disimilitud entre grupos y decrece conforme avanzan las fusiones.
- B. representa la disimilitud entre grupos y crece de forma monotónica.
- C. mide la disimilitud sin requerir comportamiento monotónico.

### Solución AAII.EX.2023SO.9

## AAII.EX.2023SO.10

### Enunciado AAII.EX.2023SO.10

La disimilitud basada en la media entre grupos (group average)…

- A. converge al mismo resultado que single linkage con datos infinitos.
- B. proporciona estimaciones consistentes del valor esperado de distancia entre grupos.
- C. genera agrupamientos con disimilitudes no necesariamente monotónicas pero consistentes.

### Solución AAII.EX.2023SO.10
