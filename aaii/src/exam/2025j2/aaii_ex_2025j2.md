<!--
SPDX-FileCopyrightText: 2026 Colaboradores de apuntes_muicd_uned

SPDX-License-Identifier: CC-BY-4.0
-->

# AAII.EX.2025J2

Ejercicios elaborados con fines educativos, inspirados en los contenidos evaluados en el exámen de la sesión de la 2.ª semana de junio 2025 de Aprendizaje Automático II del MUICD de la UNED.

Este documento no es una copia ni una transcripción del examen oficial, sino una redacción propia de ejercicios conceptualmente equivalentes.

Asignatura: Aprendizaje Automático II  
Duración máxima: 60 minutos  
Material permitido: todo tipo de material de consulta.

N.° preguntas: 10
N.° por pregunta: 4
Pregunta correcta: +1 punto
Pregunta incorrecta: -0,3 puntos

Indicaciones  
La prueba consta de 10 preguntas teórico-prácticas sobre los contenidos de la asignatura. Cada cuestión presenta hasta cuatro opciones, siendo válida únicamente una. Cada respuesta correcta suma un punto y cada respuesta incorrecta resta 0.3 puntos.

## AAII.EX.2025J2.1

### Enunciado AAII.EX.2025J2.1

Una propiedad destacada de métodos de clustering basados en densidad como DBSCAN es que…

- A. no requieren ponderar explícitamente la importancia de cada variable.
- B. producen siempre el mismo número de grupos que K-medias.
- C. permiten formas arbitrarias de clúster y no exigen fijar previamente K.
- D. garantizan convergencia global en cada iteración.

### Solución AAII.EX.2025J2.1

## AAII.EX.2025J2.2

### Enunciado AAII.EX.2025J2.2

El objetivo principal del método Bagging es…

- A. disminuir el sesgo del modelo base.
- B. incrementar la complejidad de los modelos individuales.
- C. reducir la varianza de la predicción combinada.
- D. optimizar directamente la función de pérdida cuadrática.

### Solución AAII.EX.2025J2.2

## AAII.EX.2025J2.3

### Enunciado AAII.EX.2025J2.3

Según la formulación clásica del boosting con pérdida cuadrática, cada nuevo árbol ajusta…

- A. el gradiente de los errores de clasificación.
- B. los residuos de la predicción previa, equivalentes a la derivada del error cuadrático.
- C. el promedio de las observaciones fuera de bolsa.
- D. la entropía asociada a la distribución de pesos.

### Solución AAII.EX.2025J2.3

## AAII.EX.2025J2.4

### Enunciado AAII.EX.2025J2.4

En un bosque aleatorio, la importancia de las variables suele estimarse mediante…

- A. el análisis de la matriz de confusión.
- B. criterios de pureza como Gini junto con permutaciones aleatorias.
- C. contrastes estadísticos chi-cuadrado.
- D. coeficientes de regularización específicos por variable.

### Solución AAII.EX.2025J2.4

## AAII.EX.2025J2.5

### Enunciado AAII.EX.2025J2.5

Un metamodelo lineal puede resultar insuficiente al combinar clasificadores altamente no lineales porque…

- A. ignora la presencia de variables categóricas.
- B. puede no capturar interacciones complejas entre modelos base.
- C. incrementa sistemáticamente el sesgo del stacking.
- D. no presenta limitaciones en este contexto.

### Solución AAII.EX.2025J2.5

## AAII.EX.2025J2.6

### Enunciado AAII.EX.2025J2.6

Cambiar el criterio de división en un Random Forest por uno menos habitual (por ejemplo, entropía en lugar de Gini) podría…

- A. reducir el número necesario de árboles.
- B. mejorar automáticamente la interpretabilidad.
- C. alterar las particiones, generando árboles potencialmente más diversos.
- D. exigir validación cruzada en cada árbol.

### Solución AAII.EX.2025J2.6

## AAII.EX.2025J2.7

### Enunciado AAII.EX.2025J2.7

El clustering jerárquico aglomerativo construye grupos…

- A. partiendo de un único grupo global que se subdivide.
- B. comenzando con cada observación como clúster independiente y fusionándolos.
- C. aplicando validación cruzada en cada fusión.
- D. fijando previamente el número de grupos.

### Solución AAII.EX.2025J2.7

## AAII.EX.2025J2.8

### Enunciado AAII.EX.2025J2.8

El boosting puede ser más propenso al sobreajuste que el bagging porque…

- A. entrena modelos de forma independiente.
- B. enfatiza progresivamente ejemplos difíciles, pudiendo ajustar ruido.
- C. requiere validación cruzada iterativa.
- D. utiliza menos datos de entrenamiento.

### Solución AAII.EX.2025J2.8

## AAII.EX.2025J2.9

### Enunciado AAII.EX.2025J2.9

En AdaBoost, tras cada iteración las muestras mal clasificadas…

- A. se eliminan del entrenamiento.
- B. reducen su peso relativo.
- C. incrementan su peso para recibir mayor atención.
- D. no influyen en la siguiente iteración.

### Solución AAII.EX.2025J2.9

## AAII.EX.2025J2.10

### Enunciado AAII.EX.2025J2.10

El stacking puede comportarse de forma similar al bagging cuando…

- A. el metamodelo ignora las variables originales.
- B. los modelos base son árboles podados.
- C. el metamodelo se limita a promediar directamente las predicciones.
- D. se emplean subconjuntos aleatorios de variables en cada división.

### Solución AAII.EX.2025J2.10
